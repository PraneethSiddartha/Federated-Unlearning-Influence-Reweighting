# Federated Unlearning via Lightweight Influence-Aware Reweighting

[![Paper](https://img.shields.io/badge/Paper-Springer%20Nature%20CS-blue)](https://link.springer.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)

> **Post-hoc client-level federated unlearning through influence-aware gradient subtraction**

<p align="center">
  <img src="figures/conceptual/fig1_client_influence_decomposition.png" width="80%" alt="Architecture Overview">
</p>

---

## ğŸ¯ Key Contributions

We propose a **lightweight, post-hoc method** for federated unlearning that:

| Feature | Our Method | Benefit |
|---------|------------|---------|
| âœ… **No training modification** | Works on already-trained FL models | Deploy immediately |
| âœ… **123,167Ã— speedup** | 0.015s vs 31 min retraining | Real-time compliance |
| âœ… **5.4 MB storage** | O(KÃ—\|Î¸\|) complexity | 10Ã— less than FedEraser |
| âœ… **MIA â†’ 51%** | Near random guessing | Effective forgetting |
| âœ… **<2% utility drop** | Preserves model quality | Practical deployment |

---

## ğŸ“Š Results at a Glance

### Main Results (Î± = 0.5)

| Metric | Before | After Unlearning | Retrained Baseline |
|--------|--------|------------------|-------------------|
| **MIA Accuracy** | 68.34 Â± 2.15% | **51.23 Â± 1.89%** | 50.45 Â± 1.12% |
| **Retain Accuracy** | 80.15 Â± 1.18% | 78.92 Â± 1.52% | 79.23 Â± 1.41% |
| **Forget Accuracy** | 78.67 Â± 2.34% | 18.45 Â± 3.21% | 12.38 Â± 2.87% |
| **Cosine Similarity** | - | **0.962 Â± 0.005** | 1.000 |
| **Unlearning Time** | - | **0.015s** | 1847.5s |

### Privacy-Utility Trade-off

<p align="center">
  <img src="figures/experiments/fig3_pareto_frontier.png" width="70%" alt="Pareto Frontier">
</p>

---

## ğŸ”¬ Method Overview

### Core Formula

```
Î¸áµ˜ = Î¸áµ€ - Î± Ã— Î”Î¸c
```

Where:
- `Î¸áµ€` = Trained federated model
- `Î”Î¸c` = Accumulated gradient contribution of client c
- `Î±` = Unlearning strength parameter (optimal: 0.5)
- `Î¸áµ˜` = Resulting unlearned model

### How It Works

1. **During Training**: Track each client's gradient contribution
2. **Upon Request**: Subtract target client's influence with scaling factor Î±
3. **Result**: Model behaves as if client never participated

<p align="center">
  <img src="figures/conceptual/fig10_gradient_flow_diagram.png" width="80%" alt="Gradient Flow">
</p>

---

## ğŸ“ Repository Structure

```
Federated-Unlearning-Influence-Reweighting/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ CITATION.cff                 # Citation metadata
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiment.yaml          # Experiment configuration
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py                # FL client implementation
â”‚   â”œâ”€â”€ server.py                # FL server with gradient tracking
â”‚   â””â”€â”€ unlearn.py               # Unlearning algorithm
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_figures.py      # Figure generation
â”‚   â””â”€â”€ generate_tables.py       # Table generation
â”‚
â”œâ”€â”€ figures/                     # Publication figures
â”‚   â”œâ”€â”€ conceptual/              # System diagrams
â”‚   â”œâ”€â”€ algorithm/               # Method visualizations
â”‚   â”œâ”€â”€ experiments/             # Empirical results
â”‚   â””â”€â”€ comparisons/             # Method comparisons
â”‚
â”œâ”€â”€ results/tables/              # Results in CSV/MD format
â”‚
â”œâ”€â”€ calculations/                # Complexity analysis
â”‚
â”œâ”€â”€ paper/sections/              # LaTeX paper sections
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚
â””â”€â”€ docs/                        # Documentation
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/PraneethSiddartha/Federated-Unlearning-Influence-Reweighting.git
cd Federated-Unlearning-Influence-Reweighting

# Create environment
conda create -n fedunlearn python=3.9
conda activate fedunlearn

# Install dependencies
pip install -r requirements.txt
```

### Run Experiments

```bash
# Train federated model with gradient tracking
python src/server.py --config configs/experiment.yaml

# Perform unlearning for client 3
python src/unlearn.py --target-client 3 --alpha 0.5
```

---

## ğŸ“ˆ Experimental Setup

| Parameter | Value |
|-----------|-------|
| **Dataset** | FEMNIST (62 classes) |
| **Model** | SimpleCNN (~134K params) |
| **Clients (K)** | 10 |
| **Rounds (T)** | 20 |
| **Local Epochs (E)** | 5 |
| **Learning Rate** | 0.01 |
| **Non-IID** | Dirichlet Î±=0.5 |
| **Seeds** | [0, 1, 2, 3, 4] |

---

## ğŸ“Š Method Comparison

<p align="center">
  <img src="figures/comparisons/fig7_radar_comparison.png" width="60%" alt="Method Comparison">
</p>

| Method | Speedup | Storage | Post-hoc | Training Mod |
|--------|---------|---------|----------|--------------|
| **Ours** | **123,167Ã—** | **5.4 MB** | âœ… Yes | âŒ No |
| FedEraser | 4Ã— | ~54 MB | Partial | Partial |
| FedAU | ~10â¶Ã— | ~100 MB | âŒ No | âœ… Yes |
| Retraining | 1Ã— | 0 MB | âœ… Yes | âŒ No |

---

## âš ï¸ Limitations

1. **Single Dataset**: Evaluated only on FEMNIST
2. **Lightweight Model**: SimpleCNN (~134K params) only
3. **Empirical Guarantees**: Not certified/formal unlearning
4. **Architectural Comparisons**: FedEraser/FedAU comparisons are architectural, not empirical

---

## ğŸ“š Citation

```bibtex
@article{siddartha2026fedunlearn,
  title={Federated Unlearning via Lightweight Influence-Aware Reweighting},
  author={Siddartha, Praneeth},
  journal={SN Computer Science},
  publisher={Springer Nature},
  year={2026}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ‘¤ Author

**Praneeth Siddartha**

---

<p align="center">
  <b>Target Journal:</b> Springer Nature Computer Science
</p>
